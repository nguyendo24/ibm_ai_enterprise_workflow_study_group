{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE STUDY - Deploying a recommender\n",
    "\n",
    "We have seen the movie lens data on a toy dataset now lets try something a little bigger.  You have some\n",
    "choices.\n",
    "\n",
    "* [MovieLens Downloads](https://grouplens.org/datasets/movielens/latest/)\n",
    "\n",
    "If your resources are limited (your working on a computer with limited amount of memory)\n",
    "\n",
    "> continue to use the sample_movielens_ranting.csv\n",
    "\n",
    "If you have a computer with at least 8GB of RAM\n",
    "\n",
    "> download the ml-latest-small.zip\n",
    "\n",
    "If you have the computational resources (access to Spark cluster or high-memory machine)\n",
    "\n",
    "> download the ml-latest.zip\n",
    "\n",
    "The two important pages for documentation are below.\n",
    "\n",
    "* [Spark MLlib collaborative filtering docs](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) \n",
    "* [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "## ensure the spark context is available\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "        .appName(\"sandbox\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensure the data are downloaded and specify the file paths here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['links.csv', 'tags.csv', 'ratings.csv', 'README.txt', 'movies.csv']\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(\".\", \"ml-latest-small\")\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "ratings_file = os.path.join(data_dir, \"ratings.csv\")\n",
    "movies_file = os.path.join(data_dir, \"movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the data\n",
    "df_ratings = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(ratings_file)\n",
    "df_movies = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",\"true\").load(movies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1\n",
    "\n",
    "Explore the movie lens data a little and summarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "+------+-------+------+---------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "Ratings: 100836 records with 4 columns\n",
      "\n",
      "\n",
      "Movies: 9742 records with 3 columns\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE (summarize the data)\n",
    "\n",
    "df_ratings.show(n=4)\n",
    "df_movies.show(n=4)\n",
    "\n",
    "print(f'Ratings: {df_ratings.count()} records with {len(df_ratings.columns)} columns\\n\\n')\n",
    "print(f'Movies: {df_movies.count()} records with {len(df_movies.columns)} columns\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2\n",
    "\n",
    "Find the ten most popular movies---that is the then movies with the highest average rating\n",
    "\n",
    ">Hint: you may want to subset the movie matrix to only consider movies with a minimum number of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|movieId|avg(rating)|count(userId)|\n",
      "+-------+-----------+-------------+\n",
      "|  78836|        5.0|            2|\n",
      "|   3473|        5.0|            2|\n",
      "|   6818|        5.0|            2|\n",
      "|     53|        5.0|            2|\n",
      "|   6442|        5.0|            2|\n",
      "|   1151|        5.0|            2|\n",
      "|     99|        5.0|            2|\n",
      "| 142444|        5.0|            1|\n",
      "|  26350|        5.0|            1|\n",
      "|    148|        5.0|            1|\n",
      "+-------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_ratings.groupBy(\"movieId\") \\\n",
    "    .agg(F.avg(\"rating\"), F.count(\"userId\")) \\\n",
    "    .orderBy([\"avg(rating)\", \"count(userId)\"], ascending=[0,0]) \\\n",
    "    .show(10)\n",
    "\n",
    "# SELECT movieId, AVG(rating), count(userId) GroupBy movieID  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3\n",
    "\n",
    "Compare at least 5 different values for the ``regParam``\n",
    "\n",
    "Use the `` ALS.trainImplicit()`` and compare it to the ``.fit()`` method.  See the [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n",
    "for example usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.001: RMSE = 1.124\n",
      "Lambda = 0.01: RMSE = 1.015\n",
      "Lambda = 0.1: RMSE = 0.9\n",
      "Lambda = 0.3: RMSE = 0.926\n",
      "Lambda = 0.5: RMSE = 1.004\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "(training, test) = df_ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "def train_eval_als(lambda_, implicit=False):\n",
    "    als = ALS(rank=5, \n",
    "              maxIter=5, \n",
    "              regParam = lambda_, \n",
    "              userCol=\"userId\", \n",
    "              itemCol=\"movieId\", \n",
    "              ratingCol=\"rating\", \n",
    "              coldStartStrategy=\"drop\",\n",
    "              implicitPrefs=implicit)\n",
    "    model = als.fit(training)\n",
    "    \n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f'Lambda = {lambda_}: RMSE = {str(round(rmse,3))}')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "for lambda_ in [0.001, 0.01, 0.1, 0.3, 0.5]:\n",
    "    train_eval_als(lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "With your best `regParam` try using the `implicitPrefs` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.1: RMSE = 3.249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ALS_2aa0ebe85f8d"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "train_eval_als(0.1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 5\n",
    "\n",
    "Use model persistence to save your finalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda = 0.1: RMSE = 0.9\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "model = train_eval_als(0.1)\n",
    "\n",
    "save_dir = \"saved-recommender\"\n",
    "if os.path.isdir(save_dir):\n",
    "    print(\"overwriting saved model\")\n",
    "    shutil.rmtree(save_dir)\n",
    "\n",
    "model.save(save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 6\n",
    "\n",
    "Use ``spark-submit`` to load the model and demonstrate that you can load the model and interface with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example-spark-submit.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile example-spark-submit.sh\n",
    "/usr/local/bin/spark/spark-submit \\\n",
    "--master local[*] \\\n",
    "--executor-memory 1G \\\n",
    "--driver-memory 1G \\\n",
    "$@\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: example-spark-submit.sh: not found\r\n"
     ]
    }
   ],
   "source": [
    "!example-spark-submit.sh recommender-submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
