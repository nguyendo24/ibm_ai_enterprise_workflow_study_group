{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.5\n",
      "('spark.executor.memory', '512m')\n",
      "('spark.driver.port', '33333')\n",
      "('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.readOnly', 'false')\n",
      "('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark')\n",
      "('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.path', '/root/data')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.kubernetes.pyspark.pythonVersion', '3')\n",
      "('spark.driver.memory', '512m')\n",
      "('spark.kubernetes.namespace', 'spark')\n",
      "('spark.app.name', 'spark')\n",
      "('spark.kubernetes.authenticate.serviceAccountName', 'spark')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.app.id', 'spark-application-1588942627957')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.executor.instances', '1')\n",
      "('spark.kubernetes.container.image', 'sbc1/spark-py')\n",
      "('spark.executor.cores', '1')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.master', 'k8s://https://kubernetes.default.svc.cluster.local:443')\n",
      "('spark.driver.host', 'my-notebook-deployment.spark.svc.cluster.local')\n",
      "('spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.options.claimName', 'my-notebook-pvc')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "# Create Spark config for our Kubernetes based cluster manager\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "sparkConf.setAppName(\"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"sbc1/spark-py\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"spark\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"1\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.driver.port\", \"33333\")\n",
    "sparkConf.set(\"spark.driver.host\", \"my-notebook-deployment.spark.svc.cluster.local\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.options.claimName\",\"my-notebook-pvc\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.path\",\"/root/data\")\n",
    "sparkConf.set(\"spark.kubernetes.executor.volumes.persistentVolumeClaim.my-notebook-pvc.mount.readOnly\",\"false\")\n",
    "# Initialize our Spark cluster, this will actually\n",
    "# generate the worker nodes.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "print(spark.version)\n",
    "\n",
    "for attribute in sc._conf.getAll():\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-08 13:00:36--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.64.133, 151.101.0.133, 151.101.192.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.64.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104736 (102K) [text/plain]\n",
      "Saving to: ‘sample_libsvm_data.txt’\n",
      "\n",
      "sample_libsvm_data. 100%[===================>] 102.28K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2020-05-08 13:00:37 (3.67 MB/s) - ‘sample_libsvm_data.txt’ saved [104736/104736]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-intro.pdf',\n",
       " 'calculate-pi-out.txt',\n",
       " 'docker-tutorial',\n",
       " 'c5w1-spark-intro.ipynb',\n",
       " 'sherlock-holmes.txt',\n",
       " 'c5w1-docker-tutorial.ipynb',\n",
       " 'sample_libsvm_data.txt',\n",
       " 'calculate-pi.py',\n",
       " '.python-version',\n",
       " '.ipynb_checkpoints',\n",
       " 'c5w2-spark-mllib-example.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and parse the data file, converting it to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"libsvm\").load(\"./sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 100 records with 2 columns\n",
      "\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(f'Read {data.count()} records with {len(data.columns)} columns\\n\\n')\n",
    "print(data.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index labels, adding metadata to the label column.\n",
    "### Fit on whole dataset to include all labels in index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+\n",
      "|label|            features|indexedLabel|\n",
      "+-----+--------------------+------------+\n",
      "|  0.0|(692,[127,128,129...|         1.0|\n",
      "|  1.0|(692,[158,159,160...|         0.0|\n",
      "|  1.0|(692,[124,125,126...|         0.0|\n",
      "|  1.0|(692,[152,153,154...|         0.0|\n",
      "|  1.0|(692,[151,152,153...|         0.0|\n",
      "|  0.0|(692,[129,130,131...|         1.0|\n",
      "|  1.0|(692,[158,159,160...|         0.0|\n",
      "|  1.0|(692,[99,100,101,...|         0.0|\n",
      "|  0.0|(692,[154,155,156...|         1.0|\n",
      "|  0.0|(692,[127,128,129...|         1.0|\n",
      "|  1.0|(692,[154,155,156...|         0.0|\n",
      "|  0.0|(692,[153,154,155...|         1.0|\n",
      "|  0.0|(692,[151,152,153...|         1.0|\n",
      "|  1.0|(692,[129,130,131...|         0.0|\n",
      "|  0.0|(692,[154,155,156...|         1.0|\n",
      "|  1.0|(692,[150,151,152...|         0.0|\n",
      "|  0.0|(692,[124,125,126...|         1.0|\n",
      "|  0.0|(692,[152,153,154...|         1.0|\n",
      "|  1.0|(692,[97,98,99,12...|         0.0|\n",
      "|  1.0|(692,[124,125,126...|         0.0|\n",
      "+-----+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StringIndexer` converts a string column to a numerical index column that will be treated as a categorical variable by by spark.\n",
    "\n",
    "NOTE: In this case we're basically one-hot encoding the label column. Seems silly to do it like this since label column is already one-hot encoded, but doing otherwise left as exercise..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically identify categorical features, and index them.\n",
    "### Set maxCategories so features with > 4 distinct values are treated as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|     indexedFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featureIndexer.transform(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VectorIndexer` helps process a dataset of unknown vectors into a dataset with some continuous features and some categorical features by automatically identifying categorical features. The choice between continuous and categorical is based upon the maxCategories parameter.\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-features.html#vectorindexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose 315 categorical features: 645, 69, 365, 138, 479, 333, 249, 0, 666, 88, 170, 115, 276, 308, 5, 449, 120, 614, 677, 202, 10, 56, 533, 142, 340, 670, 174, 42, 417, 24, 37, 25, 257, 389, 52, 14, 504, 110, 587, 619, 196, 559, 638, 20, 421, 46, 93, 284, 228, 448, 57, 78, 29, 475, 164, 591, 646, 253, 106, 121, 84, 147, 280, 61, 221, 396, 89, 133, 116, 1, 507, 312, 74, 307, 452, 6, 248, 60, 117, 678, 529, 85, 201, 220, 366, 534, 102, 334, 28, 38, 561, 392, 70, 424, 192, 21, 137, 165, 33, 92, 229, 252, 197, 361, 65, 97, 665, 224, 615, 9, 53, 169, 141, 420, 109, 256, 225, 339, 77, 193, 669, 476, 642, 590, 679, 96, 393, 647, 173, 13, 41, 503, 134, 73, 105, 2, 311, 558, 674, 530, 586, 618, 166, 32, 34, 148, 45, 279, 64, 17, 584, 562, 423, 191, 22, 44, 59, 118, 281, 27, 641, 71, 391, 12, 445, 54, 611, 144, 49, 335, 86, 672, 172, 113, 219, 419, 81, 362, 451, 76, 7, 39, 649, 98, 616, 477, 367, 535, 103, 140, 621, 91, 66, 251, 668, 198, 108, 278, 223, 394, 306, 135, 563, 226, 3, 505, 80, 167, 35, 473, 675, 589, 531, 255, 648, 112, 617, 194, 145, 48, 557, 63, 640, 18, 282, 95, 310, 50, 67, 199, 673, 16, 585, 502, 338, 643, 31, 336, 613, 11, 72, 446, 612, 143, 43, 250, 450, 99, 363, 87, 671, 104, 368, 588, 40, 26, 390, 55, 114, 171, 139, 418, 23, 8, 75, 119, 58, 667, 478, 82, 620, 447, 36, 168, 146, 30, 51, 19, 422, 564, 305, 107, 4, 136, 506, 79, 195, 474, 532, 94, 283, 395, 644, 47, 15, 163, 200, 68, 62, 277, 691, 501, 90, 111, 254, 227, 337, 83, 309, 560, 639, 676, 222, 592, 364\n"
     ]
    }
   ],
   "source": [
    "categoricalFeatures = featureIndexer.categoryMaps\n",
    "\n",
    "print(\"Chose %d categorical features: %s\" %\n",
    "      (len(categoricalFeatures), \", \".join(str(k) for k in categoricalFeatures.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and test sets (30% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert indexed labels back to original labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IndexToString` maps a column of label indices back to a column containing the original labels as strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and pipeline and fit on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData.select(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-----------+----------+--------------+\n",
      "|            features|     indexedFeatures|rawPrediction|probability|prediction|predictedLabel|\n",
      "+--------------------+--------------------+-------------+-----------+----------+--------------+\n",
      "|(692,[100,101,102...|(692,[100,101,102...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|(692,[121,122,123...|(692,[121,122,123...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[123,124,125...|(692,[123,124,125...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[124,125,126...|(692,[124,125,126...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|(692,[125,126,127...|(692,[125,126,127...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|(692,[126,127,128...|(692,[126,127,128...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[126,127,128...|(692,[126,127,128...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[126,127,128...|(692,[126,127,128...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|(692,[126,127,128...|(692,[126,127,128...|    [3.0,7.0]|  [0.3,0.7]|       1.0|           0.0|\n",
      "|(692,[129,130,131...|(692,[129,130,131...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|(692,[150,151,152...|(692,[150,151,152...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|(692,[151,152,153...|(692,[151,152,153...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[152,153,154...|(692,[152,153,154...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[153,154,155...|(692,[153,154,155...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|(692,[153,154,155...|(692,[153,154,155...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|(692,[154,155,156...|(692,[154,155,156...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|(692,[119,120,121...|(692,[119,120,121...|    [7.0,3.0]|  [0.7,0.3]|       0.0|           1.0|\n",
      "|(692,[124,125,126...|(692,[124,125,126...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "|(692,[125,126,127...|(692,[125,126,127...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "|(692,[126,127,128...|(692,[126,127,128...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "+--------------------+--------------------+-------------+-----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[100,101,102...|\n",
      "|  0.0|(692,[121,122,123...|\n",
      "|  0.0|(692,[123,124,125...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[125,126,127...|\n",
      "|  0.0|(692,[126,127,128...|\n",
      "|  0.0|(692,[126,127,128...|\n",
      "|  0.0|(692,[126,127,128...|\n",
      "|  0.0|(692,[126,127,128...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[119,120,121...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[125,126,127...|\n",
      "|  1.0|(692,[126,127,128...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`label`' given input columns: [rawPrediction, probability, features, predictedLabel, indexedFeatures, prediction];;\\n'Project [predictedLabel#503, 'label, features#140]\\n+- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, prediction#492, if (isnull(cast(prediction#492 as double))) null else UDF(cast(prediction#492 as double)) AS predictedLabel#503]\\n   +- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, UDF(rawPrediction#483) AS prediction#492]\\n      +- Project [features#140, indexedFeatures#480, rawPrediction#483, UDF(rawPrediction#483) AS probability#487]\\n         +- Project [features#140, indexedFeatures#480, UDF(indexedFeatures#480) AS rawPrediction#483]\\n            +- Project [features#140, UDF(features#140) AS indexedFeatures#480]\\n               +- Project [features#140]\\n                  +- Sample 0.7, 1.0, false, 8907249391665270363\\n                     +- Sort [label#139 ASC NULLS FIRST, features#140 ASC NULLS FIRST], false\\n                        +- Relation[label#139,features#140] libsvm\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1128.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`label`' given input columns: [rawPrediction, probability, features, predictedLabel, indexedFeatures, prediction];;\n'Project [predictedLabel#503, 'label, features#140]\n+- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, prediction#492, if (isnull(cast(prediction#492 as double))) null else UDF(cast(prediction#492 as double)) AS predictedLabel#503]\n   +- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, UDF(rawPrediction#483) AS prediction#492]\n      +- Project [features#140, indexedFeatures#480, rawPrediction#483, UDF(rawPrediction#483) AS probability#487]\n         +- Project [features#140, indexedFeatures#480, UDF(indexedFeatures#480) AS rawPrediction#483]\n            +- Project [features#140, UDF(features#140) AS indexedFeatures#480]\n               +- Project [features#140]\n                  +- Sample 0.7, 1.0, false, 8907249391665270363\n                     +- Sort [label#139 ASC NULLS FIRST, features#140 ASC NULLS FIRST], false\n                        +- Relation[label#139,features#140] libsvm\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-54a40213606e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Select example rows to display.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predictedLabel\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Select (prediction, true label) and compute test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m evaluator = MulticlassClassificationEvaluator(\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`label`' given input columns: [rawPrediction, probability, features, predictedLabel, indexedFeatures, prediction];;\\n'Project [predictedLabel#503, 'label, features#140]\\n+- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, prediction#492, if (isnull(cast(prediction#492 as double))) null else UDF(cast(prediction#492 as double)) AS predictedLabel#503]\\n   +- Project [features#140, indexedFeatures#480, rawPrediction#483, probability#487, UDF(rawPrediction#483) AS prediction#492]\\n      +- Project [features#140, indexedFeatures#480, rawPrediction#483, UDF(rawPrediction#483) AS probability#487]\\n         +- Project [features#140, indexedFeatures#480, UDF(indexedFeatures#480) AS rawPrediction#483]\\n            +- Project [features#140, UDF(features#140) AS indexedFeatures#480]\\n               +- Project [features#140]\\n                  +- Sample 0.7, 1.0, false, 8907249391665270363\\n                     +- Sort [label#139 ASC NULLS FIRST, features#140 ASC NULLS FIRST], false\\n                        +- Relation[label#139,features#140] libsvm\\n\""
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------+--------------------+-------------+-----------+----------+--------------+\n",
      "|label|            features|indexedLabel|     indexedFeatures|rawPrediction|probability|prediction|predictedLabel|\n",
      "+-----+--------------------+------------+--------------------+-------------+-----------+----------+--------------+\n",
      "|  0.0|(692,[100,101,102...|         1.0|(692,[100,101,102...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|  0.0|(692,[121,122,123...|         1.0|(692,[121,122,123...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[123,124,125...|         1.0|(692,[123,124,125...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[124,125,126...|         1.0|(692,[124,125,126...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|  0.0|(692,[125,126,127...|         1.0|(692,[125,126,127...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|  0.0|(692,[126,127,128...|         1.0|(692,[126,127,128...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[126,127,128...|         1.0|(692,[126,127,128...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[126,127,128...|         1.0|(692,[126,127,128...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|  0.0|(692,[126,127,128...|         1.0|(692,[126,127,128...|    [3.0,7.0]|  [0.3,0.7]|       1.0|           0.0|\n",
      "|  0.0|(692,[129,130,131...|         1.0|(692,[129,130,131...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|  0.0|(692,[150,151,152...|         1.0|(692,[150,151,152...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|  0.0|(692,[151,152,153...|         1.0|(692,[151,152,153...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[152,153,154...|         1.0|(692,[152,153,154...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[153,154,155...|         1.0|(692,[153,154,155...|   [0.0,10.0]|  [0.0,1.0]|       1.0|           0.0|\n",
      "|  0.0|(692,[153,154,155...|         1.0|(692,[153,154,155...|    [2.0,8.0]|  [0.2,0.8]|       1.0|           0.0|\n",
      "|  0.0|(692,[154,155,156...|         1.0|(692,[154,155,156...|    [1.0,9.0]|  [0.1,0.9]|       1.0|           0.0|\n",
      "|  1.0|(692,[119,120,121...|         0.0|(692,[119,120,121...|    [7.0,3.0]|  [0.7,0.3]|       0.0|           1.0|\n",
      "|  1.0|(692,[124,125,126...|         0.0|(692,[124,125,126...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "|  1.0|(692,[125,126,127...|         0.0|(692,[125,126,127...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "|  1.0|(692,[126,127,128...|         0.0|(692,[126,127,128...|   [10.0,0.0]|  [1.0,0.0]|       0.0|           1.0|\n",
      "+-----+--------------------+------------+--------------------+-------------+-----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
