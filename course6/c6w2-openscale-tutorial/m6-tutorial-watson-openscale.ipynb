{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenScale Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.8\n",
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.1.8) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/opt/conda/envs/py36/lib/python3.6/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME             MODEL            VERSION                            \n",
      "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.1.0\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__)\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's start by creating the predictive model. \n",
    "This is a text classfier to predict customer churn based on movie reviews. We'll use the same data as we did in Module 4.\n",
    "\n",
    "The input data is plain text of movie reviews. We'll first clean, tokenize and lemmatize the words, then convert them to a numeric matrix of tf-idf.\n",
    "\n",
    "We'll do this in two parts, so that the deployed model can have a numeric feature matrix as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sklearn\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from string import punctuation, printable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  label\n",
      "0  b\"arnold schwarzenegger has been an icon for a...      0\n",
      "1  b\"good films are hard to find these days . \\ng...      1\n",
      "2  b\"quaid stars as a man who has taken up the pr...      1\n",
      "3  b'we could paraphrase michelle pfieffer\\'s cha...      0\n",
      "4  b\"kolya is one of the richest films i've seen ...      1\n"
     ]
    }
   ],
   "source": [
    "movie_reviews = pd.read_csv('movie_reviews.csv')\n",
    "print(movie_reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = movie_reviews['review']\n",
    "y = movie_reviews['label'].rename('*label*')\n",
    "\n",
    "# The target column must have a name that does not appear as a word/token in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80\\'s , but lately his films have been very sloppy and the one-liners are getting worse . \\\\nit\\'s hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what\\'s it matter to him ? \\\\nonce again arnold has signed to do another expensive blockbuster , that can\\'t compare with the likes of the terminator series , true lies and even eraser . \\\\nin this so called dark thriller , the devil ( gabriel byrne ) has come upon earth , to impregnate a woman ( robin tunney ) which happens every 1000 years , and basically destroy the world , but apparently god has chosen one man , and that one man is jericho cane ( arnold himself ) . \\\\nwith the help of a trusty sidekick ( kevin pollack ) , they will stop at nothing to let the devil take over the world ! \\\\nparts of this are actually so absurd , that they would fit right in with dogma . \\\\nyes , the film is that weak , but it\\'s better than the other blockbuster right now ( sleepy hollow ) , but it makes the world is not enough look like a 4 star film . \\\\nanyway , this definitely doesn\\'t seem like an arnold movie . \\\\nit just wasn\\'t the type of film you can see him doing . \\\\nsure he gave us a few chuckles with his well known one-liners , but he seemed confused as to where his character and the film was going . \\\\nit\\'s understandable , especially when the ending had to be changed according to some sources . \\\\naside form that , he still walked through it , much like he has in the past few films . \\\\ni\\'m sorry to say this arnold but maybe these are the end of your action days . \\\\nspeaking of action , where was it in this film ? \\\\nthere was hardly any explosions or fights . \\\\nthe devil made a few places explode , but arnold wasn\\'t kicking some devil butt . \\\\nthe ending was changed to make it more spiritual , which undoubtedly ruined the film . \\\\ni was at least hoping for a cool ending if nothing else occurred , but once again i was let down . \\\\ni also don\\'t know why the film took so long and cost so much . \\\\nthere was really no super affects at all , unless you consider an invisible devil , who was in it for 5 minutes tops , worth the overpriced budget . \\\\nthe budget should have gone into a better script , where at least audiences could be somewhat entertained instead of facing boredom . \\\\nit\\'s pitiful to see how scripts like these get bought and made into a movie . \\\\ndo they even read these things anymore ? \\\\nit sure doesn\\'t seem like it . \\\\nthankfully gabriel\\'s performance gave some light to this poor film . \\\\nwhen he walks down the street searching for robin tunney , you can\\'t help but feel that he looked like a devil . \\\\nthe guy is creepy looking anyway ! \\\\nwhen it\\'s all over , you\\'re just glad it\\'s the end of the movie . \\\\ndon\\'t bother to see this , if you\\'re expecting a solid action flick , because it\\'s neither solid nor does it have action . \\\\nit\\'s just another movie that we are suckered in to seeing , due to a strategic marketing campaign . \\\\nsave your money and see the world is not enough for an entertaining experience . \\\\n\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    arnold schwarzenegger icon action enthusiast l...\n",
      "1    good film hard day ngreat film rare nproof lif...\n",
      "2    quaid star man proffesion dragonslayer pron fe...\n",
      "3    pron paraphrase michelle pfieffer character da...\n",
      "4    kolya rich film pron time nzdenek sverak play ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# clean up and convert the plain text\n",
    "good_characters = 'abcdefghijklmnopqrstuvwxyz '\n",
    "def lemmatize(doc):\n",
    "    if doc[0] == 'b':\n",
    "        doc = doc[1:]  # get rid of leading b\n",
    "    doc = str(doc).lower()\n",
    "    doc = \"\".join([char for char in doc if char in good_characters])\n",
    "    doc = nlp(doc)\n",
    "    tokens = [re.sub(\"\\W+\",\"\",token.lemma_.lower()) for token in doc ]\n",
    "    return ' '.join(w for w in tokens if (w not in ENGLISH_STOP_WORDS) and (len(w)>1))\n",
    "\n",
    "X = X.apply(lemmatize)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    daylight sylvester stallone break new ground c...\n",
      "1    walt disney studio finally meet pron match lus...\n",
      "2    pron giant begin monologue funny distinctive p...\n",
      "3    country legal rule law principle seriously for...\n",
      "4    brainless teen flick surprise drug sex nstar k...\n",
      "Name: review, dtype: object 0    welcome pron ohso typical sequel nit try twice...\n",
      "1    television gay character infamous ellen sitcom...\n",
      "2    note consider portion following text spoiler n...\n",
      "3    pron thirty year later oscar felix nthat excit...\n",
      "4    nearly film tim burton direct homage horror ge...\n",
      "Name: review, dtype: object 0    1\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    0\n",
      "Name: *label*, dtype: int64 0    0\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: *label*, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = [data.reset_index(drop = True) for \n",
    "                                    data in train_test_split(X,y, random_state = 101)]\n",
    "\n",
    "print(X_train.head(), X_test.head(), y_train.head(), y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer(max_features = 10000).fit(X_train)\n",
    "X_train_tfidf = tf_idf.transform(X_train).todense()\n",
    "X_test_tfidf = tf_idf.transform(X_test).todense()\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron' 'abandon' 'abby' ... 'zorro' 'zucker' 'zwick'] 10000\n"
     ]
    }
   ],
   "source": [
    "# The vocabulary in the tf_idf is an unordered dictionary.\n",
    "# we need to sort it so that we can associate words with coefficients\n",
    "words = []\n",
    "indices = []\n",
    "for word, index in tf_idf.vocabulary_.items():\n",
    "    words.append(word)\n",
    "    indices.append(index)\n",
    "    \n",
    "sorted_index = np.argsort(indices)\n",
    "sorted_words = np.array(words)[sorted_index]\n",
    "print(sorted_words, len(sorted_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abby</th>\n",
       "      <th>abduction</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abigail</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>ably</th>\n",
       "      <th>aboard</th>\n",
       "      <th>...</th>\n",
       "      <th>zetajone</th>\n",
       "      <th>zinger</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zwick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandon  abby  abduction  aberdeen  abigail  ability  able  ably  \\\n",
       "0    0.0      0.0   0.0        0.0       0.0      0.0      0.0   0.0   0.0   \n",
       "1    0.0      0.0   0.0        0.0       0.0      0.0      0.0   0.0   0.0   \n",
       "2    0.0      0.0   0.0        0.0       0.0      0.0      0.0   0.0   0.0   \n",
       "3    0.0      0.0   0.0        0.0       0.0      0.0      0.0   0.0   0.0   \n",
       "4    0.0      0.0   0.0        0.0       0.0      0.0      0.0   0.0   0.0   \n",
       "\n",
       "   aboard  ...  zetajone  zinger  zoe    zombie  zone  zoolander  zoom  zorro  \\\n",
       "0     0.0  ...       0.0     0.0  0.0  0.000000   0.0        0.0   0.0    0.0   \n",
       "1     0.0  ...       0.0     0.0  0.0  0.100692   0.0        0.0   0.0    0.0   \n",
       "2     0.0  ...       0.0     0.0  0.0  0.000000   0.0        0.0   0.0    0.0   \n",
       "3     0.0  ...       0.0     0.0  0.0  0.000000   0.0        0.0   0.0    0.0   \n",
       "4     0.0  ...       0.0     0.0  0.0  0.000000   0.0        0.0   0.0    0.0   \n",
       "\n",
       "   zucker  zwick  \n",
       "0     0.0    0.0  \n",
       "1     0.0    0.0  \n",
       "2     0.0    0.0  \n",
       "3     0.0    0.0  \n",
       "4     0.0    0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the tf-idf (numeric) array to a pandas dataframe with feature columns\n",
    "X_train_df = pd.DataFrame(X_train_tfidf)\n",
    "X_test_df = pd.DataFrame(X_test_tfidf)\n",
    "X_train_df.columns = sorted_words\n",
    "X_test_df.columns = sorted_words\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have converted our plain text to a dense DataFrame of tf-idf for the top 10,000 terms. \n",
    "\n",
    "### You will notice that nearly all of the cells are zero, and many of the words don't seem to be relevant to whether or not a customer is likely to churn. Words like \"awful\" and \"wonderful\" are likely correlated with customer satisfaction, but the implcation of words like 'zombie' and 'zoolander' are not as obvious.\n",
    "\n",
    "### To reduce the complexity of the model, we will identify the 200 most significant words and rebuild a model just using these. Note that *most significant* does not mean most common -- it means the words that have the highest magnitude coefficient in the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  1.0\n",
      "Test score:  0.836\n",
      "Confusion Matrix: \n",
      " [[204  46]\n",
      " [ 36 214]]\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(loss = 'log', max_iter = 1000).fit(X_train_df, y_train)\n",
    "print('Training score: ', model.score(X_train_df, y_train))\n",
    "print('Test score: ', model.score(X_test_df, y_test))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, model.predict(X_test_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most positive words: \n",
      " ['great' 'fun' 'war' 'hilarious' 'matrix' 'life' 'good' 'performance'\n",
      " 'quite' 'terrific' 'excellent' 'truman' 'memorable' 'perfect' 'trek'\n",
      " 'frank' 'job' 'solid' 'wonderful' 'town']\n",
      "20 most negative words: \n",
      " ['bad' 'waste' 'plot' 'suppose' 'attempt' 'bore' 'script' 'fail' 'stupid'\n",
      " 'minute' 'boring' 'nunfortunately' 'lame' 'nif' 'tv' 'poor' 'try' 'dull'\n",
      " 'ridiculous' 'talk']\n"
     ]
    }
   ],
   "source": [
    "indices_of_positive_features = np.argsort(model.coef_)[0][::-1][:100]\n",
    "indices_of_negative_features = np.argsort(model.coef_)[0][:100]\n",
    "\n",
    "vocabulary = np.array(sorted_words)\n",
    "positive_vocab = vocabulary[indices_of_positive_features]\n",
    "negative_vocab = vocabulary[indices_of_negative_features]\n",
    "print('20 most positive words: \\n', positive_vocab[:20])\n",
    "print('20 most negative words: \\n', negative_vocab[:20])\n",
    "significant_vocabulary = np.hstack([positive_vocab, negative_vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of signficant words\n",
    "\n",
    "A quick check of the word clouds for positive and negative words gives us confidence that our model is working as expected:\n",
    "\n",
    "### Positive Wordcloud\n",
    "![positive wordcloud](images/positive_wordcloud.png)\n",
    "\n",
    "\n",
    "### Negative Wordcloud\n",
    "![negative_wordcloud](images/negative_wordcloud.png)\n",
    "\n",
    "In terms of *Business Opportunity* these findings are not unexecpted. This provides important validation of a mathematical and measureable link between customer sentiment and business opportunities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having found the most siginficant words, let us now build a TF-IDF Vectorizer focused only on these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.08226093 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02981901 0.03842041 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.0536962  ... 0.         0.         0.        ]\n",
      " [0.         0.06351697 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(1500, 200)\n"
     ]
    }
   ],
   "source": [
    "significant_tfidf = TfidfVectorizer(vocabulary = significant_vocabulary).fit(X_train)\n",
    "X_train_significant_tfidf = significant_tfidf.transform(X_train).todense()\n",
    "X_test_significant_tfidf = significant_tfidf.transform(X_test).todense()\n",
    "print(X_train_significant_tfidf)\n",
    "print(X_train_significant_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this trained TF-IDF Vectorizer for future use\n",
    "with open('significant_tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(significant_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>great</th>\n",
       "      <th>fun</th>\n",
       "      <th>war</th>\n",
       "      <th>hilarious</th>\n",
       "      <th>matrix</th>\n",
       "      <th>life</th>\n",
       "      <th>good</th>\n",
       "      <th>performance</th>\n",
       "      <th>quite</th>\n",
       "      <th>terrific</th>\n",
       "      <th>...</th>\n",
       "      <th>thought</th>\n",
       "      <th>seagal</th>\n",
       "      <th>appear</th>\n",
       "      <th>director</th>\n",
       "      <th>clich</th>\n",
       "      <th>batman</th>\n",
       "      <th>lifeless</th>\n",
       "      <th>devil</th>\n",
       "      <th>spawn</th>\n",
       "      <th>eddie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029819</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028622</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056952</td>\n",
       "      <td>0.074507</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17256</td>\n",
       "      <td>0.040787</td>\n",
       "      <td>0.029846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028556</td>\n",
       "      <td>0.099762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      great       fun       war  hilarious  matrix      life      good  \\\n",
       "0  0.000000  0.082261  0.000000        0.0     0.0  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000        0.0     0.0  0.000000  0.000000   \n",
       "2  0.029819  0.038420  0.000000        0.0     0.0  0.028622  0.000000   \n",
       "3  0.029750  0.000000  0.712646        0.0     0.0  0.028556  0.099762   \n",
       "4  0.000000  0.000000  0.000000        0.0     0.0  0.000000  0.000000   \n",
       "\n",
       "   performance     quite  terrific  ...  thought   seagal    appear  director  \\\n",
       "0     0.000000  0.079763       0.0  ...      0.0  0.00000  0.000000  0.000000   \n",
       "1     0.000000  0.240373       0.0  ...      0.0  0.00000  0.000000  0.000000   \n",
       "2     0.056952  0.074507       0.0  ...      0.0  0.17256  0.040787  0.029846   \n",
       "3     0.000000  0.000000       0.0  ...      0.0  0.00000  0.000000  0.000000   \n",
       "4     0.000000  0.000000       0.0  ...      0.0  0.00000  0.000000  0.266995   \n",
       "\n",
       "   clich  batman  lifeless  devil  spawn  eddie  \n",
       "0    0.0     0.0       0.0    0.0    0.0    0.0  \n",
       "1    0.0     0.0       0.0    0.0    0.0    0.0  \n",
       "2    0.0     0.0       0.0    0.0    0.0    0.0  \n",
       "3    0.0     0.0       0.0    0.0    0.0    0.0  \n",
       "4    0.0     0.0       0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_significant_df = pd.DataFrame(X_train_significant_tfidf)\n",
    "X_test_significant_df = pd.DataFrame(X_test_significant_tfidf)\n",
    "X_train_significant_df.columns = significant_tfidf.vocabulary_\n",
    "X_test_significant_df.columns = significant_tfidf.vocabulary_\n",
    "X_train_significant_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the training data, with `*label*` column.\n",
    "\n",
    "## We will need this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([X_train_significant_df, y_train], axis =1).to_csv('training_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now train a SGDClassifier using this smaller, more select data set and see how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score:  0.918\n",
      "Test score:  0.818\n",
      "Confusion Matrix: \n",
      " [[204  46]\n",
      " [ 45 205]]\n"
     ]
    }
   ],
   "source": [
    "significant_model = SGDClassifier(loss = 'log', max_iter = 1000).fit(X_train_significant_df, y_train)\n",
    "print('Training score: ', significant_model.score(X_train_significant_df, y_train))\n",
    "print('Test score: ', significant_model.score(X_test_significant_df, y_test))\n",
    "print('Confusion Matrix: \\n', confusion_matrix(y_test, significant_model.predict(X_test_significant_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the test data, this preforms almost exactly as well as the orignal model which had 10,000 feature words, but this one has only 200 feature words -- it uses 98% less data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a reasonably good model, let's deploy it to IBM Watson ML\n",
    "You should already have an account with Watson Studio and credentials saved in ```~/.ibm/wml.json```.\n",
    "\n",
    "If you need to create these credentials, please revisit the tutorial in Module 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['apikey', 'instance_id', 'url'])\n"
     ]
    }
   ],
   "source": [
    "# Import credentials from ~/.ibm/wml.json\n",
    "wmlcreds_file = os.path.join(os.path.expanduser(\"~\"),'.ibm/wml.json')\n",
    "# or set location of file manually:\n",
    "wmlcreds_file = wmlcreds_file #replace with location of file if different than ~/.ibm/wml.json\n",
    "with open(wmlcreds_file, \"r\") as wml_file:\n",
    "        wmlcreds = json.load(wml_file)\n",
    "\n",
    "# convert credentials to the correct dictoinary format\n",
    "wml_credentials = {\"apikey\": wmlcreds['apikey'],\n",
    "    \"instance_id\": wmlcreds['instance_id'],\n",
    "    \"url\": wmlcreds['url'],}\n",
    "\n",
    "print(wml_credentials.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a client to interact with Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<watson_machine_learning_client.client.WatsonMachineLearningAPIClient object at 0x7f21dc0c1978>\n"
     ]
    }
   ],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review deployed models and delete any unused models to make room for new deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ---------------------  ------------------------  -----------------\n",
      "GUID                                  NAME                   CREATED                   FRAMEWORK\n",
      "00292b90-9595-4632-a478-6033f0923809  GermanCreditRiskModel  2020-05-23T15:41:05.977Z  mllib-2.3\n",
      "fcf06b97-4556-4e93-9859-6725c083b0d7  Model200               2020-05-23T15:33:42.691Z  scikit-learn-0.20\n",
      "------------------------------------  ---------------------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "client.repository.list_models()\n",
    "# guid = 'Insert guid to delete here'\n",
    "# client.repository.delete(guid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's store the Model in the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  --------  ------------------------  -----------------\n",
      "GUID                                  NAME      CREATED                   FRAMEWORK\n",
      "fcf06b97-4556-4e93-9859-6725c083b0d7  Model200  2020-05-23T15:33:42.691Z  scikit-learn-0.20\n",
      "------------------------------------  --------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "metadata = {client.repository.ModelMetaNames.NAME: \"Model200\",\n",
    "client.repository.ModelMetaNames.FRAMEWORK_NAME: \"scikit-learn\",\n",
    "client.repository.ModelMetaNames.FRAMEWORK_VERSION: sklearn.__version__}\n",
    "model_details = client.repository.store_model(significant_model, meta_props=metadata)\n",
    "\n",
    "model_uid = client.repository.get_model_uid(model_details)\n",
    "\n",
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now we deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "Synchronous deployment creation for uid: 'fcf06b97-4556-4e93-9859-6725c083b0d7' started\n",
      "\n",
      "#######################################################################################\n",
      "\n",
      "\n",
      "INITIALIZING\n",
      "DEPLOY_SUCCESS\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "Successfully finished deployment creation, deployment_uid='27bba833-a4ec-4f27-a73d-9ec18688be93'\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "27bba833-a4ec-4f27-a73d-9ec18688be93\n",
      "DEPLOY_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "deployment_details = client.deployments.create(artifact_uid=model_uid, \n",
    "                                               name=\"Model1-deployment\")\n",
    "deployment_uid = deployment_details['metadata']['guid']\n",
    "print(deployment_uid)\n",
    "print(client.deployments.get_status(deployment_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  -----------------  ------------------------  -----------------  -----------------\n",
      "GUID                                  NAME               CREATED                   FRAMEWORK          TYPE\n",
      "fcf06b97-4556-4e93-9859-6725c083b0d7  Model200           2020-05-23T15:33:42.691Z  scikit-learn-0.20  model\n",
      "27bba833-a4ec-4f27-a73d-9ec18688be93  Model1-deployment  2020-05-23T15:33:50.478Z  scikit-learn-0.20  online deployment\n",
      "------------------------------------  -----------------  ------------------------  -----------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "# We should now see a model and the corresponding online deployment\n",
    "client.repository.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's verify the deployment by sending it some data and getting a response\n",
    "\n",
    "Let's send it a movie review and see if it thinks we are likely to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_movie_review = \"This movie is really good. I really liked it. I want to watch it again. Excellent!\"\n",
    "# movie_review = \"bad hated terrible awful horrible never\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize and tf-idf the text.\n",
    "As before, we can't just send plain text like this. It needs to be cleaned, lemmatized and converted to a tf-idf numeric matrix, using the same methods that the model was first trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   great  fun  war  hilarious  matrix  life      good  performance  quite  \\\n",
      "0    0.0  0.0  0.0        0.0     0.0   0.0  0.288594          0.0    0.0   \n",
      "\n",
      "   terrific  ...  thought  seagal  appear  director  clich  batman  lifeless  \\\n",
      "0       0.0  ...      0.0     0.0     0.0       0.0    0.0     0.0       0.0   \n",
      "\n",
      "   devil  spawn  eddie  \n",
      "0    0.0    0.0    0.0  \n",
      "\n",
      "[1 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "good_movie_review = lemmatize(good_movie_review)\n",
    "good_movie_review = significant_tfidf.transform([good_movie_review]).todense()\n",
    "good_movie_review = pd.DataFrame(good_movie_review)\n",
    "good_movie_review.columns = significant_tfidf.vocabulary_\n",
    "print(good_movie_review.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now create a data payload and send this to the deployed model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': ['prediction', 'probability'], 'values': [[1, [0.02243592418041085, 0.9775640758195892]]]}\n"
     ]
    }
   ],
   "source": [
    "values = good_movie_review.values.tolist()\n",
    "payload = {'fields':list(significant_tfidf.vocabulary_), 'values': values}\n",
    "scoring_url = deployment_details['entity']['scoring_url']\n",
    "prediction = client.deployments.score(scoring_url, payload)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-south.ml.cloud.ibm.com/v3/wml_instances/1765fb96-97be-4e7d-a2c6-5fcef771ee19/deployments/27bba833-a4ec-4f27-a73d-9ec18688be93/online'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the same with a bad movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   great  fun  war  hilarious  matrix  life  good  performance  quite  \\\n",
      "0    0.0  0.0  0.0        0.0     0.0   0.0   0.0          0.0    0.0   \n",
      "\n",
      "   terrific  ...  thought  seagal  appear  director  clich  batman  lifeless  \\\n",
      "0       0.0  ...      0.0     0.0     0.0       0.0    0.0     0.0       0.0   \n",
      "\n",
      "   devil  spawn  eddie  \n",
      "0    0.0    0.0    0.0  \n",
      "\n",
      "[1 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "bad_review = \"I really hated this bad movie. it was awful. I want to quit. Horrible. terrible\"\n",
    "bad_review = lemmatize(bad_review)\n",
    "bad_review = significant_tfidf.transform([bad_review]).todense()\n",
    "bad_review = pd.DataFrame(bad_review)\n",
    "bad_review.columns = significant_tfidf.vocabulary_\n",
    "print(bad_review.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': ['prediction', 'probability'], 'values': [[0, [0.9993134096701984, 0.0006865903298016069]]]}\n"
     ]
    }
   ],
   "source": [
    "values = bad_review.values.tolist()\n",
    "payload = {'fields':list(significant_tfidf.vocabulary_), 'values': values}\n",
    "scoring_url = deployment_details['entity']['scoring_url']\n",
    "prediction = client.deployments.score(scoring_url, payload)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, we get a very different prediction for the bad review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenScale Configuration\n",
    "## The next step is to link this deployed model to IBM Watson OpenScale to provide monitoring services\n",
    "\n",
    "To get started, navigate to your `Dashboard` at https://cloud.ibm.com/\n",
    "\n",
    "At the upper right, click on `Create Resource`. This will take you to the catalog of services offered in the IBM Watson Suite. We'll be using ``Watson OpenScale``, but you might want to take a minute to examine some of the many other offerings.\n",
    "\n",
    "You can also use this link to take you directly to the OpenScale service: \n",
    "\n",
    "https://cloud.ibm.com/catalog/services/watson-openscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the service\n",
    "If you're using the free lite plan, accept the default. You may change the `Service name` if you wish. In this example, I am calling my service `'OS-tutorial'`\n",
    "\n",
    "On the right, click `Create` and then `'Launch Applicaton'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the service\n",
    "You should now see the welcome message as shown below.\n",
    "![Welcome message](images/os_tutorial_1.png)\n",
    "\n",
    "\n",
    "Click on `Auto setup`\n",
    "\n",
    "It will take a few minutes as OpenScale configures some default services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Auto setup is complete, you should see the following dialog:\n",
    "![All set](images/os_tutorial_2.png)\n",
    "\n",
    "Click on `start tour`\n",
    "\n",
    "The tour shows the German Credit Risk model as an example. You may spend some time exploring this. This example address issues such as fairness by gender, which is an important consideration in many models, especially where historically different demographic groups have been discriminated against.\n",
    "\n",
    "In this model, the effect of gender or other demographic features is monitored by creating synthetic inputs in which all the data is the same except the demographic value has been altered, and then determining if a more favorable outcome would have resulted. This is equivalent to calculating the partial derivative with respect to the demographic value at a particular point in the response curve.\n",
    "\n",
    "It can then suggest, and even auto-generate models that are demonstrable more fair.\n",
    "\n",
    "OpenScale can also look for, and automatically address, selection bias and demographic imbalance in the training data set.\n",
    "\n",
    "On a a more granular level, OpenScale can examine the decision-making process behind individual transactions, provide *explainability* for the outcome and assess fairness. It can also provide a list of the minimum changes necessary to alter the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done examing the Credit Risk example, we will delete it and replace it with our churn-prediction model. Our model does not have a feature describing gender, race or other demographics, so some of the issues of fairness are not relevant.\n",
    "\n",
    "Return to the Insights Dashboard and click on the *heart beat* (zig-zag) icon at the upper left.\n",
    "![insights dashboard](images/os_tutorial_4.png)\n",
    "\n",
    "You should see the tile for the GermanCreditRiskModel_Application\n",
    "\n",
    "Click on the drop-down list and select `Remove Application`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, go to the `Model Monitors` tab where you will see the tile 'GermanCreditRisk'. Again, select 'Remove Deployment' from the drop down list on this tile.\n",
    "\n",
    "You should now see an empty Insights Dashboard and a message that says, \"Add a deployed model to get started.\"\n",
    "\n",
    "Click on `Add`\n",
    "\n",
    "In the dialog box, select the churn prediction model we deployed previously in this tutorial and click `configure`.\n",
    "\n",
    "On the next screen, click `configure monitors`.\n",
    "\n",
    "Our data type is `numeric/categorical`, which may be the only option available, and our algorithim type is `Binary classification`.\n",
    "\n",
    "Click `save`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Payload Logging\n",
    "On the next screen, you should see the following message about Payload logging.\n",
    "![payload logging](images/os_tutorial_5.png).\n",
    "\n",
    "OpenScale needs to see an example of what the incoming data is expected to look like. So we need to send some example payloads for configuration.\n",
    "\n",
    "Also on this page, you will see the details of the deployment and subscription and be able to download them as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a scoring request\n",
    "To send a score request, we need several things, a url of the deployed model, a client to send the data from python to the model and a payload or set of input data.\n",
    "\n",
    "We created the WatsonMachineLearningAPIClient earlier in this tutorial\n",
    "\n",
    "We'll use the same payload and scoring url as above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': ['prediction', 'probability'], 'values': [[0, [0.9993134096701984, 0.0006865903298016069]]]}\n"
     ]
    }
   ],
   "source": [
    "# Send the payload for scoring and print the results\n",
    "predictions = client.deployments.score(scoring_url, payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return results\n",
    "When our payload has been sent sucessfully, we should see the results returned. These include the predictions (in this case, 1 for the first review and 0 for the second review), along with the corresponding predicted probabilities.\n",
    "\n",
    "When this has been completed, return to the `Payload Logging` page and click `I'm finished` to complete the process.\n",
    "\n",
    "You should then see the message, `Logging Activated Sucessfully`\n",
    "\n",
    "![logging activated sucessfully](images/os_tutorial_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details Setup\n",
    "\n",
    "You will now need to complete the model details setup.\n",
    "\n",
    "Click on `model details` at the left and then, on the next screen, click `begin`\n",
    "\n",
    "You have two options, `Manually configure monitors` and `Upload training data distribution`. We will manually configure the monitors.\n",
    "\n",
    "Select this option and click `next`. \n",
    "![Upload Training Data](images/os_tutorial_7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the location of the training data.\n",
    "\n",
    "### We now need to create a Cloud Storage Object to store the training data.\n",
    "\n",
    "You may review your list of resources using the following link:\n",
    "https://cloud.ibm.com/resources\n",
    "\n",
    "If you do not have a Cloud Object Storage, navigate to:\n",
    "https://cloud.ibm.com/catalog\n",
    "\n",
    "Click `Storage` from the left column and then the tile for `Object Storage`\n",
    "![Cloud Object Storage](images/os_tutorial_8.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `lite` plan, provide a Service Name (optional), and then click `Create`\n",
    "\n",
    "On the next page, click `Create Bucket`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the option to create a **Custom Bucket**\n",
    "![custom_bucket](images/os_tutorial_9.png)\n",
    "\n",
    "Provide a unique name (for example, one containing your username) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Under `Resiliency` make sure you select \"Cross Region\"\n",
    "\n",
    "### Under `Location` select your region.\n",
    "\n",
    "### You may leave the remaining options as default\n",
    "\n",
    "### Click `Create Bucket` at the lower right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Training Data\n",
    "\n",
    "You should now see an empty bucket and a prompt to drag and drop files to upload.\n",
    "![drag and drop files](images/os_tutorial_10.png) \n",
    "\n",
    "Locate the file `training_data.csv` that we created earlier in this tutorial. (It should be located in the same directory as this notebook.)  Drag and drop this to the bucket.\n",
    "\n",
    "It should only take a few moments to upload this data.\n",
    "\n",
    "### Leave this window open. You will need it for connection data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect the Training Data to OpenScale\n",
    "\n",
    "We now need to link the training data in the Cloud Storage bucket to our model in OpenScale.\n",
    "\n",
    "Return to the OpenScale Dashboard. Under `Model Details` it requests, \"Specify the location of the training data.\"\n",
    "\n",
    "Select `Cloud Object Storage` from the drop-down list.\n",
    "\n",
    "To find the `Login URL`, return to the Cloud Object Storage webpage and select `Endpoint` from the column at left.\n",
    "\n",
    "In the drop-down list for `Resiliency`, select `Cross Region` and select your Location from the locations drop-down.\n",
    "![endpoint url](images/os_tutorial_11.png)\n",
    "\n",
    "Select and copy the url for your endpoint. (In this tutorial, I am using us-geo, Public).\n",
    "\n",
    "Return to the webpage for OpenScale and paste this url into `login url`.\n",
    "\n",
    "### Important: you must add \"https://\" to the front of this url."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource Instance ID\n",
    "\n",
    "To locate your `Resource Instance ID`, return to the Cloud Storage webpage and select `Service Credentials` from the left column.\n",
    "\n",
    "Next, select `New Credentials`.\n",
    "\n",
    "On the `Add New Credential` dialog box, change the Role to `Manager` and click `Add`.\n",
    "\n",
    "When the credentials appear, select `View credentials` under the `Actions` column.\n",
    "\n",
    "You should now see a JSON object including \"apikey\", \"endpoints\", etc.\n",
    "\n",
    "In this JSON, copy the value for resource_instance_id.  Paste this into the `Resource Instance ID` field in the OpenScale webpage.\n",
    "\n",
    "Do the same for the \"apikey\" value in the JSON.\n",
    "\n",
    "When these values have been entered into the OpenScale configuration, select `Test`.\n",
    "\n",
    "After a couple of seconds, you should see: \n",
    "![success](images/os_tutorial_12.png)\n",
    "\n",
    "Click `Next` at the lower right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select your bucket and training data set.\n",
    "\n",
    "From the drop-down lists on the next page, select the bucket and training data set you have uploaded, and then click `Next`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Configuration\n",
    "\n",
    "### OpenScale will now review the training data and associate the column labels.\n",
    "\n",
    "Rememeber that our label column is called `*label*`, so as not to conflict with the word \"label\" if it were to appear in the corpus. OpenScale automatically identifies this column as the label column and asks to confirm. \n",
    "\n",
    "Select this column name and click `Next`\n",
    "\n",
    "![select label data](images/os_tutorial_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the features used to train the AI Deployment\n",
    "\n",
    "![feature columns](images/os_tutorial_14.png)\n",
    "\n",
    "\n",
    "OpenScale again automatically identifies the *feature* columns. Make sure these are all selected and click `Next`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select text and categorical features\n",
    "\n",
    "In this model, we do not have any text or categorical features. All of our features have been reduced to a TF-IDF numeric matrix. \n",
    "\n",
    "Leave all these features unselected and click `Next`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the deployment prediction column\n",
    "\n",
    "This refers to the predictions created by our model *not* to a `predictions` column in our data set. Select the `predictions` option and click `Next`.\n",
    "\n",
    "Do the same for `probabiltiy` on the following page.\n",
    "\n",
    "We did not include a transcation ID in this model, so this optional column is left blank.\n",
    "\n",
    "On the final page, review the **Models details summary** and click `Save`\n",
    "\n",
    "After a few seconds, it should say:\n",
    "![model ready](images/os_tutorial_15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Drift Monitor\n",
    "\n",
    "The input to our model is plain text. We have not collected demographic information such as race, gender or nationality, so the Fairness monitoring tool is not applicable to this tutorial. \n",
    "\n",
    "We will use the Drift monitoring tool to monitor the deployed model over time.\n",
    "\n",
    "Select `Drift` from the column at left and then `Begin` at lower right\n",
    "![drift](images/os_tutorial_drift1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure accuracy drift monitor\n",
    "\n",
    "Select the option for `Analyze and train in Watson OpenScale` and click `Next`\n",
    "\n",
    "![drift source](images/os_tutorial_drift2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the drift alert threshold\n",
    "    \n",
    "A drift alert threshold of 10% is the defualt. Lower values will make the system more sensitive to small amounts of drift, providing early warning of changes to data.\n",
    "\n",
    "Click `Next`\n",
    "\n",
    "Set the sample size on the next page and click `Next`\n",
    "\n",
    "Review the settings for the Drift monitor on the final page and click `Save`.\n",
    "\n",
    "It may take ten to twenty minutes to configure and train the drift monitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Monitoring\n",
    "\n",
    "After the Drift monitor is trained, you can return to the IBM Watson OpenScale dashboard and monitor the performance of the model over time as additional transactions are made.\n",
    "\n",
    "Note that quality and fairness metrics update once per hour and drift updates once every three hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fields': ['prediction', 'probability'], 'values': [[0, [0.9993134096701984, 0.0006865903298016069]]]}\n"
     ]
    }
   ],
   "source": [
    "# Send the payload for scoring and print the results\n",
    "predictions = client.deployments.score(scoring_url, payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
